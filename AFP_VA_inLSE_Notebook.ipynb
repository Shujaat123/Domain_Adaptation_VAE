{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFP_VA_inLSE_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMs72HYbZyHS1aRJfBI90qJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/Domain_Adaptation_VAE/blob/main/AFP_VA_inLSE_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEJa2_TZnec"
      },
      "source": [
        "## **AFP-VA-inLSE: Antifreeze Proteins Prediction Using instance-normalized Latent Space Encoding of Composition of k-Spaced Amino Acid Pairs**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-m_byicZkQc"
      },
      "source": [
        "This code provide python implementation of AFP-LSE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadaflfshqgF"
      },
      "source": [
        "# Loading Useful packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GTv8il0Swd0",
        "outputId": "a19efd3d-873b-40c9-d2cb-684411ad0038"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6012nzZktl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "a8fa90d2-3eef-4480-923d-5e9c1a58e3c0"
      },
      "source": [
        "## Load useful packages\n",
        "import sys, os, re, gc\n",
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.layers import InstanceNormalization, GroupNormalization\n",
        "\n",
        "\n",
        "from tensorflow.keras import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-8123382c140c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstanceNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGroupNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LayerNormalization' from 'tensorflow_addons.layers' (/usr/local/lib/python3.7/dist-packages/tensorflow_addons/layers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwrowDzZ_Ah"
      },
      "source": [
        "# Feature-Extraction\n",
        "\n",
        "The CKSAAP feature encoding calculates the frequency of amino acid pairs separated by any k residues. The CKSAAP encoding scheme reflects the amino acid pair information in small and large range with in the peptides depending upon the value of k(gap). The encoding scheme is utilized from iFeature web server, using the following download link: (https://github.com/Superzchen/iFeature).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p258z8dGR0iQ"
      },
      "source": [
        "## Define CKSAAP feature-extraction function\n",
        "def minSequenceLength(fastas):\n",
        "\tminLen = 10000\n",
        "\tfor i in fastas:\n",
        "\t\tif minLen > len(i[1]):\n",
        "\t\t\tminLen = len(i[1])\n",
        "\treturn minLen\n",
        "\n",
        "def CKSAAP(fastas, gap=5, **kw):\n",
        "\tif gap < 0:\n",
        "\t\tprint('Error: the gap should be equal or greater than zero' + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tif minSequenceLength(fastas) < gap+2:\n",
        "\t\tprint('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n\\n')\n",
        "\t\treturn 0\n",
        "\n",
        "\tAA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\tencodings = []\n",
        "\taaPairs = []\n",
        "\tfor aa1 in AA:\n",
        "\t\tfor aa2 in AA:\n",
        "\t\t\taaPairs.append(aa1 + aa2)\n",
        "\theader = ['#']\n",
        "\tfor g in range(gap+1):\n",
        "\t\tfor aa in aaPairs:\n",
        "\t\t\theader.append(aa + '.gap' + str(g))\n",
        "\tencodings.append(header)\n",
        "\tfor i in fastas:\n",
        "\t\tname, sequence = i[0], i[1]\n",
        "\t\tcode = [name]\n",
        "\t\tfor g in range(gap+1):\n",
        "\t\t\tmyDict = {}\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tmyDict[pair] = 0\n",
        "\t\t\tsum = 0\n",
        "\t\t\tfor index1 in range(len(sequence)):\n",
        "\t\t\t\tindex2 = index1 + g + 1\n",
        "\t\t\t\tif index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\n",
        "\t\t\t\t\tmyDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
        "\t\t\t\t\tsum = sum + 1\n",
        "\t\t\tfor pair in aaPairs:\n",
        "\t\t\t\tcode.append(myDict[pair] / sum)\n",
        "\t\tencodings.append(code)\n",
        "\treturn encodings\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYTVJzDNbo_"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "        "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9mtHsEaAyA"
      },
      "source": [
        "# Designing an Auto-Encoder-based classifier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge7uF0idR-Gc"
      },
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def AFP_LSE_Final_Model(LV=5):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(3600,), name='enc_input')\n",
        "    enc_l1 = Dense(50, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\n",
        "\n",
        "    enc_l2 = Dense(25, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\n",
        "\n",
        "    # enc_l3 = Dense(10, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    # enc_l3 = BatchNormalization()(enc_l3)\n",
        "    # enc_l3 = Dropout(rate = 0.3)(enc_l3)\n",
        "\n",
        "    # enc_l4 = Dense(10, activation='relu', name='encoder_layer4')(enc_l3)\n",
        "    # enc_l4 = BatchNormalization()(enc_l4)\n",
        "    # enc_l4 = Dropout(rate = 0.3)(enc_l4)\n",
        "\n",
        "    # encoder_output = Dense(LV, activation='sigmoid', name='encoder_output')(enc_l4)\n",
        "    z_mean = Dense(LV, name=\"z_mean\")(enc_l2)\n",
        "    z_log_var = Dense(LV, name=\"z_log_var\")(enc_l2)\n",
        "    encoder_output = Sampling()([z_mean, z_log_var])\n",
        "    # encoder_output = InstanceNormalization()(encoder_output)\n",
        "    # encoder_output = GroupNormalization(groups = int(LV/2))(encoder_output)\n",
        "    encoder_output = GroupNormalization(groups = 1)(encoder_output)\n",
        "\n",
        "\n",
        "  \n",
        "    # # Classifier Network\n",
        "    # class_l1 = Dense(10, activation='relu', name='class_layer1')(encoder_output)\n",
        "    # class_l2 = Dense(10, activation='relu', name='class_layer2')(class_l1)\n",
        "    # # class_l3 = Dense(10, activation='relu', name='class_layer3')(class_l2)\n",
        "    # # class_output = Dense(2, activation='softmax', name='class_output')(class_l3)\n",
        "    # class_output = Dense(2, activation='softmax', name='class_output')(class_l2)\n",
        "\n",
        "    # Decoder Network\n",
        "    # encoder_output_c = InstanceNormalization()(encoder_output)\n",
        "    # dec_l1 = Dense(10, activation='relu', name='decoder_layer1')(encoder_output_c)\n",
        "    ##############\n",
        "    # dec_l1 = Dense(10, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    # dec_l1 = BatchNormalization()(dec_l1)\n",
        "    # dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    # encoder_output_c = InstanceNormalization()(encoder_output)\n",
        "    # dec_l2 = Dense(25, activation='relu', name='decoder_layer1')(encoder_output_c)\n",
        "    dec_l2 = Dense(25, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    # dec_l2 = Dense(25, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    dec_l3 = Dense(50, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    # dec_l4 = Dense(50, activation='relu', name='decoder_layer4')(dec_l3)\n",
        "    # dec_l4 = BatchNormalization()(dec_l4)\n",
        "    # dec_l4 = Dropout(rate = 0.3)(dec_l4)\n",
        "\n",
        "    decoder_output = Dense(3600, activation='sigmoid', name='decoder_output')(dec_l3)\n",
        "\n",
        "\n",
        "    # Classifier Network\n",
        "    # encoder_output_c = InstanceNormalization()(decoder_output)\n",
        "    # class_l1 = Dense(10, activation='relu', name='class_layer1')(decoder_output)\n",
        "    class_l1 = Dense(10, activation='relu', name='class_layer1')(encoder_output)\n",
        "    class_l2 = Dense(10, activation='relu', name='class_layer2')(class_l1)\n",
        "    class_output = Dense(2, activation='softmax', name='class_output')(class_l2)\n",
        "\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[class_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss={'class_output': 'binary_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  loss_weights={'class_output': 0.5, 'decoder_output': 0.5},\n",
        "                  metrics=[metrics.categorical_accuracy])\n",
        "    # Here I used rmsprops optimizer with default values, two objective functions are optimized\n",
        "    # using  weight factors [1 for classifier and 0.1 for decoder loss]\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83mfGGGKaBdj"
      },
      "source": [
        "## Define performance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsirQcmlTK7y"
      },
      "source": [
        "## Define performance measures\n",
        "def yoden_index(y, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "  j = (tp/(tp+fn)) + (tn/(tn+fp)) - 1\n",
        "  return j\n",
        "\n",
        "def pmeasure(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    f1score = 2 * tp / (2 * tp + fp + fn)\n",
        "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})\n",
        "\n",
        "def Show_Statistics(msg,Stats):\n",
        "  print(msg.upper())\n",
        "  print(70*'-')\n",
        "  print('Accuracy:',Stats[0])\n",
        "  print('Sensitivity:',Stats[1])\n",
        "  print('Specificity:',Stats[2])\n",
        "  print('F1-Score:',Stats[3])\n",
        "  print('MCC:',Stats[4])\n",
        "  print('Balance Accuracy:',Stats[5])\n",
        "  print('Youden-Index:',Stats[6])\n",
        "  print('Reconstruction MSE:',Stats[7])\n",
        "  print(70*'-')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7frgGLSzaC1p"
      },
      "source": [
        "## Loading and pre-processing AFP prediction dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbQo93M8Obss"
      },
      "source": [
        "## Loading and pre-processing AFP prediction dataset\n",
        "data_path = 'https://raw.githubusercontent.com/Shujaat123/AFP-LSE/master/Dataset.csv'\n",
        "dataset = pd.read_csv(data_path, index_col=None)\n",
        "\n",
        "seq=[]\n",
        "\n",
        "for index, row in dataset.iterrows():\n",
        "  array = [row['Class'], row['Sequence']]\n",
        "  name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n",
        "  seq.append([name, sequence])\n",
        "\n",
        "\n",
        "cksaapfea = []\n",
        "for i in seq:\n",
        "  temp= CKSAAP([i], gap=8)\n",
        "  cksaapfea.append(temp)\n",
        "\n",
        "dt = []\n",
        "for i in range(len(cksaapfea)):\n",
        "  temp = cksaapfea[i][1][1:]\n",
        "  dt.append(temp)\n",
        "\n",
        "dtn = np.array(dt)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y = dataset['Class']\n",
        "y[y=='AFP']=1\n",
        "y[y=='NON-AFP']=0\n",
        "y = to_categorical(y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gZ--pixd9sk"
      },
      "source": [
        "RandData_path = 'https://raw.githubusercontent.com/Shujaat123/AFP-LSE/master/rand_seq.csv'\n",
        "RandDataset = pd.read_csv(RandData_path, index_col=None)\n",
        "\n",
        "seq=[]\n",
        "\n",
        "for index, row in RandDataset.iterrows():\n",
        "  array = [row['Class'], row['Sequence']]\n",
        "  name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n",
        "  seq.append([name, sequence])\n",
        "\n",
        "\n",
        "cksaapfea = []\n",
        "for i in seq:\n",
        "  temp= CKSAAP([i], gap=8)\n",
        "  cksaapfea.append(temp)\n",
        "\n",
        "randdt = []\n",
        "for i in range(len(cksaapfea)):\n",
        "  temp = cksaapfea[i][1][1:]\n",
        "  randdt.append(temp)\n",
        "\n",
        "RandDTN = np.array(randdt)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "ry = RandDataset['Class']\n",
        "ry[ry=='AFP']=1\n",
        "ry[ry=='NON-AFP']=0\n",
        "ry = to_categorical(ry)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooB2PHJbaD16"
      },
      "source": [
        "  ## Perform Monte-Carlos Simulations for [num_Trials]\\# of independent Trials\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW4-Y7y-Q28O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144f02e3-2070-4f0d-bc78-01dd702c7950"
      },
      "source": [
        "  ## Perform Monte-Carlos Simulations for [num_Trials]# of independent Trials\n",
        "LVs = [24]\n",
        "num_Trails = 5\n",
        "\n",
        "Stats =[]\n",
        "\n",
        "for loop_ind in range(0,num_Trails):\n",
        "    # from random import sample\n",
        "\n",
        "    plist = list(np.asarray(np.where(y[:,1]==1)).flatten())\n",
        "    nlist = list(np.asarray(np.where(y[:,1]==0)).flatten())\n",
        "\n",
        "    ## train\n",
        "    p_train = sample(plist, 270)\n",
        "    n_train = sample(nlist, 270)\n",
        "    train_list = p_train + n_train\n",
        "    X_train = dtn[p_train + n_train]\n",
        "    y_train = y[p_train + n_train]\n",
        "\n",
        "    ## valid\n",
        "    p_val_list = set(plist) - set(p_train)\n",
        "    n_val_list = set(nlist) - set(n_train)\n",
        "    p_val = sample(p_val_list, 30)\n",
        "    n_val = sample(n_val_list, 30)\n",
        "    X_val = dtn[list(p_val)+list(n_val)]\n",
        "    y_val = y[list(p_val)+list(n_val)]\n",
        "    val_list = list(p_val) + list(n_val)\n",
        "\n",
        "    ## test\n",
        "    dev_list = train_list + val_list\n",
        "    test_list = set(list(np.where(y)[0])) - (set(dev_list))\n",
        "    X_test = dtn[list(test_list)]\n",
        "    y_test = y[list(test_list)]\n",
        "\n",
        "    X_test_Rand = RandDTN\n",
        "    y_test_Rand = ry\n",
        "\n",
        "    model = AFP_LSE_Final_Model(LV=LVs[0])\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=100)\n",
        "    # es = EarlyStopping(monitor='val_class_output_categorical_accuracy', mode='max', verbose=0, patience=100)\n",
        "\n",
        "    checkpoint = ModelCheckpoint('models\\\\model-best.h5',\n",
        "                                  verbose=0, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "    \n",
        "\n",
        "    history = model.fit({'enc_input': X_train},\n",
        "                        {'class_output': y_train, 'decoder_output': X_train},\n",
        "                        validation_data = ({'enc_input': X_val},\n",
        "                        {'class_output': y_val, 'decoder_output': X_val}),\n",
        "                        epochs=5000, batch_size=540, callbacks=[checkpoint, es], verbose=0)\n",
        "\n",
        "    del model  # deletes the existing model\n",
        "    model = load_model('models\\\\model-best.h5', custom_objects={'Sampling':Sampling})\n",
        "\n",
        "    y_train_pred, X_train_pred = model.predict(X_train,batch_size=540, verbose=0)\n",
        "    y_train_pred = to_categorical(y_train_pred.argmax(axis=1))\n",
        "    MSE_X_train_pred = (np.square(X_train_pred - X_train)).mean(axis=1)\n",
        "\n",
        "    y_val_pred, X_val_pred = model.predict(X_val,batch_size=60, verbose=0)\n",
        "    y_val_pred = to_categorical(y_val_pred.argmax(axis=1))\n",
        "    MSE_X_val_pred = (np.square(X_val_pred - X_val)).mean(axis=1)\n",
        "\n",
        "    y_test_pred, X_test_pred = model.predict(X_test,batch_size=9074, verbose=0)\n",
        "    y_test_pred = to_categorical(y_test_pred.argmax(axis=1))\n",
        "    MSE_X_test_pred = (np.square(X_test_pred - X_test)).mean(axis=1)\n",
        "\n",
        "    y_test_pred_Rand, X_test_pred_Rand = model.predict(X_test_Rand,batch_size=9074, verbose=0)\n",
        "    y_test_pred_Rand = to_categorical(y_test_pred_Rand.argmax(axis=1))\n",
        "    MSE_X_test_pred_Rand = (np.square(X_test_pred_Rand - X_test_Rand)).mean(axis=1)\n",
        "    \n",
        "    ## Training Measures\n",
        "    tr_acc = accuracy_score(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "    tr_sen = pmeasure(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))['Sensitivity']\n",
        "    tr_spe = pmeasure(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))['Specificity']\n",
        "    tr_f1 = pmeasure(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))['F1-Score']\n",
        "    tr_mcc = matthews_corrcoef(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "    tr_bacc = balanced_accuracy_score(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "    tr_yi = yoden_index(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
        "\n",
        "    ## Validation Measures\n",
        "    v_acc = accuracy_score(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "    v_sen = pmeasure(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))['Sensitivity']\n",
        "    v_spe = pmeasure(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))['Specificity']\n",
        "    v_f1 = pmeasure(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))['F1-Score']\n",
        "    v_mcc = matthews_corrcoef(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "    v_bacc = balanced_accuracy_score(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "    v_yi = yoden_index(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
        "\n",
        "    ## Test Measures\n",
        "    t_acc = accuracy_score(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "    t_sen = pmeasure(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))['Sensitivity']\n",
        "    t_spe = pmeasure(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))['Specificity']\n",
        "    t_f1 = pmeasure(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))['F1-Score']\n",
        "    t_mcc = matthews_corrcoef(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "    t_bacc = balanced_accuracy_score(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "    t_yi = yoden_index(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
        "\n",
        "    Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi, -10*np.log10(MSE_X_train_pred.mean()),\n",
        "                  v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi, -10*np.log10(MSE_X_val_pred.mean()),\n",
        "                  t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, -10*np.log10(MSE_X_test_pred.mean())])\n",
        "    print('LV=',LVs[0],'Trial:',loop_ind, 'Test Youden-index:', t_yi, 'MCC:', t_mcc, 'MSE_X_test_pred:', -10*np.log10(MSE_X_test_pred.mean()),\n",
        "          'Biasness:', 2*np.abs(0.5-np.mean(y_test_pred_Rand.argmax(axis=1))))\n",
        "\n",
        "Statistics = np.asarray(Stats)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LV= 24 Trial: 0 Test Youden-index: 0.06276935429491459 MCC: 0.021836920568564875 MSE_X_test_pred: 8.085095987303896 Biasness: 0.6464646464646464\n",
            "LV= 24 Trial: 1 Test Youden-index: 0.11328160448768076 MCC: 0.03309907804024445 MSE_X_test_pred: 6.791579630151484 Biasness: 0.39249639249639245\n",
            "LV= 24 Trial: 2 Test Youden-index: 0.5500377719535583 MCC: 0.1951962256343042 MSE_X_test_pred: 23.868950320756458 Biasness: 0.2063492063492064\n",
            "LV= 24 Trial: 3 Test Youden-index: 0.5937264300906346 MCC: 0.24710953098930963 MSE_X_test_pred: 26.575489218540117 Biasness: 0.46897546897546893\n",
            "LV= 24 Trial: 4 Test Youden-index: 0.7294374232616336 MCC: 0.3235931592168173 MSE_X_test_pred: 33.96444636150389 Biasness: 0.18759018759018764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddwzm0NGaFsy"
      },
      "source": [
        "## Show Classification/Reconstruction Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSXF9ZkXXv3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5569ccbc-30c7-4ee1-a3dc-513c43a3a0d6"
      },
      "source": [
        "## Show Classification/Reconstruction Statistics\n",
        "Show_Statistics('Training Results (MEAN)',Statistics.mean(axis=0)[0:8])\n",
        "Show_Statistics('Validation Results (MEAN)',Statistics.mean(axis=0)[8:16])\n",
        "Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[16:24])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.7944444444444445\n",
            "Sensitivity: 0.8874074074074073\n",
            "Specificity: 0.7014814814814815\n",
            "F1-Score: 0.8307542901562577\n",
            "MCC: 0.6021831935126315\n",
            "Balance Accuracy: 0.7944444444444445\n",
            "Youden-Index: 0.5888888888888888\n",
            "Reconstruction MSE: 19.993748618213413\n",
            "----------------------------------------------------------------------\n",
            "VALIDATION RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.6633333333333333\n",
            "Sensitivity: 0.7333333333333333\n",
            "Specificity: 0.5933333333333334\n",
            "F1-Score: 0.6944263436368698\n",
            "MCC: 0.3370643213191879\n",
            "Balance Accuracy: 0.6633333333333333\n",
            "Youden-Index: 0.32666666666666666\n",
            "Reconstruction MSE: 19.912806323588317\n",
            "----------------------------------------------------------------------\n",
            "TEST RESULTS (MEAN)\n",
            "----------------------------------------------------------------------\n",
            "Accuracy: 0.6326434819714104\n",
            "Sensitivity: 0.7801104972375691\n",
            "Specificity: 0.6297400195801154\n",
            "F1-Score: 0.13219273352526828\n",
            "MCC: 0.1641669828898481\n",
            "Balance Accuracy: 0.7049252584088422\n",
            "Youden-Index: 0.4098505168176844\n",
            "Reconstruction MSE: 19.85711230365117\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}